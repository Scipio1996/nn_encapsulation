Experiment: base_101_v10_2
------------ Test Options -----------------
KL_factor: 0.1
KL_manner: 1
add_cap_BN_relu: False
add_cap_dropout: False
b_init: zero
base_save_folder: result
batch_size_test: 128
batch_size_train: 128
beta1: 0.9
cap_N: 3
cap_model: v0
dataset: cifar10
debug_mode: False
depth: 14
do_squash: False
draw_hist: False
dropout_p: 0.2
experiment_name: base_101_v10_2
fc_time: 1
file_name: result/base_101_v10_2/opt_train_val_START_epoch_0_END_300.txt
fix_m: False
gamma: 0.1
look_into_details: False
loss_form: margin
lr: 0.0001
manual_seed: -1
max_epoch: 300
momentum: 0.9
multi_crop_test: False
no_visdom: False
non_target_j: False
num_workers: 2
optim: adam
phase: train_val
port_id: 8000
pre_ch_num: 32
primary_cap_num: 32
random_seed: 8969
route_num: 3
save_epoch: 25
save_folder: result/base_101_v10_2
schedule: [150, 200, 250]
scheduler: None
show_freq: 100
show_test_after_epoch: 100
squash_manner: paper
use_KL: False
use_cuda: True
use_instanceBN: False
use_multiple: False
w_version: v3
weight_decay: 0.0005
------------------ End --------------------
CapsNet (
  (tranfer_conv): Conv2d(3, 32, kernel_size=(9, 9), stride=(2, 2), padding=(1, 1)), weights=((32, 3, 9, 9), (32,)), parameters=7808
  (tranfer_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True), weights=((32,), (32,)), parameters=64
  (tranfer_relu): ReLU (inplace), weights=(), parameters=0
  (tranfer_conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2)), weights=((32, 32, 3, 3), (32,)), parameters=9248
  (tranfer_bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True), weights=((32,), (32,)), parameters=64
  (tranfer_relu1): ReLU (inplace), weights=(), parameters=0
  (cap_layer): CapLayer (
    (fc1): Linear (1152 -> 1152)
    (relu1): ReLU (inplace)
    (drop1): Dropout (p = 0.5)
    (fc2): Linear (1152 -> 10)
  ), weights=((1152, 1152), (1152,), (10, 1152), (10,)), parameters=1339786
)
Total param num # 5.176430 Mb

init learning rate 0.0001000000 at iter 0

[base_101_v10_2]	epoch/iter [0/300][0/391] ||	Loss: 1932540.6250, Top1_err: 88.2812, Top5_err: 40.6250 ||	Data/batch time: 0.2681/1.0923
[base_101_v10_2]	epoch/iter [0/300][100/391] ||	Loss: 997029.5248, Top1_err: 89.2868, Top5_err: 48.4143 ||	Data/batch time: 0.0368/0.0537
[base_101_v10_2]	epoch/iter [0/300][200/391] ||	Loss: 777355.0250, Top1_err: 88.7088, Top5_err: 47.4619 ||	Data/batch time: 0.0340/0.0470
[base_101_v10_2]	epoch/iter [0/300][300/391] ||	Loss: 658946.7295, Top1_err: 88.3461, Top5_err: 46.2105 ||	Data/batch time: 0.0334/0.0449
[base_101_v10_2]	epoch/iter [0/300][390/391] ||	Loss: 583912.7638, Top1_err: 88.1420, Top5_err: 45.7540 ||	Data/batch time: 0.0329/0.0437
Summary	epoch/iter [0/300] ||	TRAIN, Top1_err: 88.1420, Top5_err: 45.7540 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

model saved at result/base_101_v10_2/epoch_1.pth
[base_101_v10_2]	epoch/iter [1/300][0/391] ||	Loss: 231644.5469, Top1_err: 83.5938, Top5_err: 41.4062 ||	Data/batch time: 0.2182/0.2330
[base_101_v10_2]	epoch/iter [1/300][100/391] ||	Loss: 265208.7953, Top1_err: 86.9895, Top5_err: 42.9533 ||	Data/batch time: 0.0345/0.0424
[base_101_v10_2]	epoch/iter [1/300][200/391] ||	Loss: 241840.9191, Top1_err: 86.7693, Top5_err: 42.4324 ||	Data/batch time: 0.0330/0.0413
[base_101_v10_2]	epoch/iter [1/300][300/391] ||	Loss: 222995.3495, Top1_err: 86.7032, Top5_err: 42.5275 ||	Data/batch time: 0.0326/0.0409
[base_101_v10_2]	epoch/iter [1/300][390/391] ||	Loss: 208892.2816, Top1_err: 86.7980, Top5_err: 42.6080 ||	Data/batch time: 0.0325/0.0406
Summary	epoch/iter [1/300] ||	TRAIN, Top1_err: 86.7980, Top5_err: 42.6080 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

[base_101_v10_2]	epoch/iter [2/300][0/391] ||	Loss: 182814.5000, Top1_err: 87.5000, Top5_err: 51.5625 ||	Data/batch time: 0.2001/0.2184
[base_101_v10_2]	epoch/iter [2/300][100/391] ||	Loss: 141480.0999, Top1_err: 87.3917, Top5_err: 43.1080 ||	Data/batch time: 0.0335/0.0422
[base_101_v10_2]	epoch/iter [2/300][200/391] ||	Loss: 132222.1851, Top1_err: 87.0880, Top5_err: 43.0892 ||	Data/batch time: 0.0329/0.0414
[base_101_v10_2]	epoch/iter [2/300][300/391] ||	Loss: 123179.9977, Top1_err: 86.9498, Top5_err: 42.5664 ||	Data/batch time: 0.0331/0.0413
