Experiment: base_101_v2
------------ Test Options -----------------
KL_factor: 0.1
KL_manner: 1
add_cap_dropout: False
b_init: zero
base_save_folder: result
batch_size_test: 128
batch_size_train: 128
beta1: 0.9
cap_N: 3
cap_model: v0
dataset: cifar10
debug_mode: False
depth: 14
do_squash: False
draw_hist: False
dropout_p: 0.2
experiment_name: base_101_v2
file_name: result/base_101_v2/opt_train_val_START_epoch_0_END_300.txt
fix_m: False
gamma: 0.1
has_relu_in_W: False
look_into_details: False
loss_form: margin
lr: 0.0001
manual_seed: -1
max_epoch: 300
momentum: 0.9
multi_crop_test: False
no_visdom: False
non_target_j: False
num_workers: 2
optim: rmsprop
phase: train_val
port_id: 8000
random_seed: 1707
route_num: 3
save_epoch: 25
save_folder: result/base_101_v2
schedule: [150, 200, 250]
scheduler: None
show_freq: 100
show_test_after_epoch: 100
squash_manner: paper
use_KL: False
use_cuda: True
use_multiple: False
w_version: v2
weight_decay: 0.0005
------------------ End --------------------
CapsNet (
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (layer1): Sequential (
    (0): BasicBlock (
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (1): BasicBlock (
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer2): Sequential (
    (0): BasicBlock (
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential (
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock (
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer3): Sequential (
    (0): BasicBlock (
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential (
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock (
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (tranfer_conv): Conv2d(3, 256, kernel_size=(9, 9), stride=(2, 2), padding=(1, 1))
  (tranfer_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (tranfer_relu): ReLU (inplace)
  (tranfer_conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
  (cap_layer): CapLayer (
    (W): Conv2d(256, 5120, kernel_size=(1, 1), stride=(1, 1), groups=32)
  )
)

init learning rate 0.0001000000 at iter 0

[base_101_v2]	epoch/iter [0/300][0/391] ||	Loss: 3.1721, Top1_err: 83.5938, Top5_err: 42.9688 ||	Data/batch time: 0.2163/0.9037
[base_101_v2]	epoch/iter [0/300][100/391] ||	Loss: 0.8924, Top1_err: 82.8357, Top5_err: 36.3629 ||	Data/batch time: 0.0041/0.1093
[base_101_v2]	epoch/iter [0/300][200/391] ||	Loss: 0.6992, Top1_err: 78.5098, Top5_err: 30.3016 ||	Data/batch time: 0.0032/0.1059
[base_101_v2]	epoch/iter [0/300][300/391] ||	Loss: 0.6248, Top1_err: 75.5944, Top5_err: 27.2192 ||	Data/batch time: 0.0030/0.1052
[base_101_v2]	epoch/iter [0/300][390/391] ||	Loss: 0.5876, Top1_err: 73.9180, Top5_err: 25.3900 ||	Data/batch time: 0.0030/0.1058
Summary	epoch/iter [0/300] ||	TRAIN, Top1_err: 73.9180, Top5_err: 25.3900 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

model saved at result/base_101_v2/epoch_1.pth
[base_101_v2]	epoch/iter [1/300][0/391] ||	Loss: 0.4525, Top1_err: 71.0938, Top5_err: 17.9688 ||	Data/batch time: 0.2497/0.3313
[base_101_v2]	epoch/iter [1/300][100/391] ||	Loss: 0.4477, Top1_err: 66.1665, Top5_err: 17.9301 ||	Data/batch time: 0.0053/0.1118
[base_101_v2]	epoch/iter [1/300][200/391] ||	Loss: 0.4437, Top1_err: 65.1819, Top5_err: 17.0126 ||	Data/batch time: 0.0037/0.1087
[base_101_v2]	epoch/iter [1/300][300/391] ||	Loss: 0.4400, Top1_err: 64.4363, Top5_err: 16.4504 ||	Data/batch time: 0.0036/0.1076
[base_101_v2]	epoch/iter [1/300][390/391] ||	Loss: 0.4371, Top1_err: 63.9680, Top5_err: 16.1080 ||	Data/batch time: 0.0032/0.1076
Summary	epoch/iter [1/300] ||	TRAIN, Top1_err: 63.9680, Top5_err: 16.1080 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

[base_101_v2]	epoch/iter [2/300][0/391] ||	Loss: 0.4084, Top1_err: 64.8438, Top5_err: 12.5000 ||	Data/batch time: 0.1930/0.2718
[base_101_v2]	epoch/iter [2/300][100/391] ||	Loss: 0.4209, Top1_err: 60.9762, Top5_err: 14.5421 ||	Data/batch time: 0.0041/0.1083
[base_101_v2]	epoch/iter [2/300][200/391] ||	Loss: 0.4163, Top1_err: 60.2146, Top5_err: 14.0197 ||	Data/batch time: 0.0031/0.1082
