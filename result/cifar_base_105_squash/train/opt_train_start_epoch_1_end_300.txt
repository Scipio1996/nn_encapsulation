Experiment: cifar_base_105_squash
------------ Training Options -------------
KL_factor: 0.1
KL_manner: 1
add_cap_dropout: False
b_init: zero
beta1: 0.8
cap_N: 3
dataset: cifar100
debug: True
deploy: False
do_squash: True
draw_hist: False
dropout_p: 0.2
epochs: 300
experiment_name: cifar_base_105_squash
file_name: result/cifar_base_105_squash/train/opt_train_start_epoch_1_end_300.txt
fix_m: False
gamma: 0.1
has_relu_in_W: False
look_into_details: False
lr: 0.01
manual_seed: 7650
max_epoch: 300
model_cifar: capsule
momentum: 0.9
multi_crop_test: True
non_target_j: False
num_workers: 2
optim: sgd
phase: train
port: 4000
route_num: 4
save_epoch: 20
save_folder: result/cifar_base_105_squash/train
schedule_cifar: [150, 225]
scheduler: None
send_images_to_visdom: False
show_freq: 5
show_test_after_epoch: -1
skip_pre_squash: False
skip_pre_transfer: False
start_epoch: 1
test_batch: 128
test_only: False
train_batch: 128
use_CE_loss: False
use_KL: False
use_cuda: True
use_spread_loss: False
visdom: True
w_version: v3
weight_decay: 0.0005
------------------ End --------------------
CapsNet (
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (layer1): Sequential (
    (0): BasicBlock (
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (1): BasicBlock (
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (2): BasicBlock (
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer2): Sequential (
    (0): BasicBlock (
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential (
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock (
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (2): BasicBlock (
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer3): Sequential (
    (0): BasicBlock (
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential (
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock (
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (2): BasicBlock (
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (tranfer_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1))
  (tranfer_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (tranfer_relu): ReLU (inplace)
  (cap_layer): CapLayer (
    (avgpool): AvgPool2d (size=6, stride=6, padding=0, ceil_mode=False, count_include_pad=True)
    (fc): Linear (256 -> 1600)
  )
)

init learning rate 0.010000 at iter 0

Train [cifar_base_105_squash]	epoch [0/300]	iter [0/391]		data: 0.086s | batch: 0.681s	loss: 39.93205	acc: 0.00000	acc5: 4.68750
Train [cifar_base_105_squash]	epoch [0/300]	iter [5/391]		data: 0.017s | batch: 0.141s	loss: 39.91823	acc: 1.43229	acc5: 4.94792
Train [cifar_base_105_squash]	epoch [0/300]	iter [10/391]		data: 0.011s | batch: 0.091s	loss: 39.88733	acc: 1.27841	acc5: 5.11364
Train [cifar_base_105_squash]	epoch [0/300]	iter [15/391]		data: 0.012s | batch: 0.074s	loss: 39.72980	acc: 1.26953	acc5: 4.83398
Train [cifar_base_105_squash]	epoch [0/300]	iter [20/391]		data: 0.012s | batch: 0.064s	loss: 36.95124	acc: 1.15327	acc5: 4.72470
Train [cifar_base_105_squash]	epoch [0/300]	iter [25/391]		data: 0.011s | batch: 0.058s	loss: 30.97296	acc: 1.02163	acc5: 4.77764
Train [cifar_base_105_squash]	epoch [0/300]	iter [30/391]		data: 0.011s | batch: 0.054s	loss: 26.79189	acc: 1.03327	acc5: 4.88911
Train [cifar_base_105_squash]	epoch [0/300]	iter [35/391]		data: 0.011s | batch: 0.051s	loss: 23.59642	acc: 1.01997	acc5: 4.88281
Train [cifar_base_105_squash]	epoch [0/300]	iter [40/391]		data: 0.011s | batch: 0.049s	loss: 21.16973	acc: 1.00991	acc5: 4.82088
Train [cifar_base_105_squash]	epoch [0/300]	iter [45/391]		data: 0.012s | batch: 0.047s	loss: 19.19073	acc: 1.00204	acc5: 4.87432
Train [cifar_base_105_squash]	epoch [0/300]	iter [50/391]		data: 0.012s | batch: 0.046s	loss: 17.51178	acc: 0.98039	acc5: 4.88664
Train [cifar_base_105_squash]	epoch [0/300]	iter [55/391]		data: 0.012s | batch: 0.045s	loss: 16.09160	acc: 0.96261	acc5: 4.86886
Train [cifar_base_105_squash]	epoch [0/300]	iter [60/391]		data: 0.012s | batch: 0.045s	loss: 14.91965	acc: 0.93494	acc5: 4.84119
Train [cifar_base_105_squash]	epoch [0/300]	iter [65/391]		data: 0.012s | batch: 0.044s	loss: 13.90550	acc: 0.91146	acc5: 4.82955
Train [cifar_base_105_squash]	epoch [0/300]	iter [70/391]		data: 0.012s | batch: 0.043s	loss: 13.03214	acc: 0.89129	acc5: 4.76452
Train [cifar_base_105_squash]	epoch [0/300]	iter [75/391]		data: 0.012s | batch: 0.042s	loss: 12.29608	acc: 0.88405	acc5: 4.75946
Train [cifar_base_105_squash]	epoch [0/300]	iter [80/391]		data: 0.012s | batch: 0.042s	loss: 11.63032	acc: 0.87770	acc5: 4.81289
Train [cifar_base_105_squash]	epoch [0/300]	iter [85/391]		data: 0.012s | batch: 0.041s	loss: 11.05051	acc: 0.88118	acc5: 4.84193
Train [cifar_base_105_squash]	epoch [0/300]	iter [90/391]		data: 0.012s | batch: 0.041s	loss: 10.54460	acc: 0.88427	acc5: 4.79911
Train [cifar_base_105_squash]	epoch [0/300]	iter [95/391]		data: 0.012s | batch: 0.040s	loss: 10.07907	acc: 0.92773	acc5: 4.88281
Train [cifar_base_105_squash]	epoch [0/300]	iter [100/391]		data: 0.012s | batch: 0.040s	loss: 9.66406	acc: 0.92822	acc5: 4.95050
Train [cifar_base_105_squash]	epoch [0/300]	iter [105/391]		data: 0.012s | batch: 0.040s	loss: 9.28133	acc: 0.93603	acc5: 5.02653
Train [cifar_base_105_squash]	epoch [0/300]	iter [110/391]		data: 0.012s | batch: 0.040s	loss: 8.94388	acc: 0.93609	acc5: 5.01126
Train [cifar_base_105_squash]	epoch [0/300]	iter [115/391]		data: 0.012s | batch: 0.039s	loss: 8.61453	acc: 0.93615	acc5: 5.00404
Train [cifar_base_105_squash]	epoch [0/300]	iter [120/391]		data: 0.012s | batch: 0.039s	loss: 8.31157	acc: 0.94912	acc5: 4.98450
Train [cifar_base_105_squash]	epoch [0/300]	iter [125/391]		data: 0.012s | batch: 0.039s	loss: 8.03517	acc: 0.94866	acc5: 5.02232
Train [cifar_base_105_squash]	epoch [0/300]	iter [130/391]		data: 0.012s | batch: 0.039s	loss: 7.78722	acc: 0.94823	acc5: 5.01551
Train [cifar_base_105_squash]	epoch [0/300]	iter [135/391]		data: 0.012s | batch: 0.039s	loss: 7.56062	acc: 0.93635	acc5: 5.03217
Train [cifar_base_105_squash]	epoch [0/300]	iter [140/391]		data: 0.012s | batch: 0.038s	loss: 7.38353	acc: 0.96410	acc5: 5.01995
Train [cifar_base_105_squash]	epoch [0/300]	iter [145/391]		data: 0.012s | batch: 0.038s	loss: 7.18942	acc: 0.98994	acc5: 5.06207
Train [cifar_base_105_squash]	epoch [0/300]	iter [150/391]		data: 0.012s | batch: 0.038s	loss: 7.00991	acc: 1.00890	acc5: 5.07554
Train [cifar_base_105_squash]	epoch [0/300]	iter [155/391]		data: 0.012s | batch: 0.038s	loss: 6.85356	acc: 1.00661	acc5: 5.06811
Train [cifar_base_105_squash]	epoch [0/300]	iter [160/391]		data: 0.012s | batch: 0.038s	loss: 6.70319	acc: 0.98991	acc5: 5.06114
Train [cifar_base_105_squash]	epoch [0/300]	iter [165/391]		data: 0.012s | batch: 0.037s	loss: 6.55831	acc: 1.01186	acc5: 5.09224
Train [cifar_base_105_squash]	epoch [0/300]	iter [170/391]		data: 0.012s | batch: 0.037s	loss: 6.41418	acc: 1.01425	acc5: 5.12610
Train [cifar_base_105_squash]	epoch [0/300]	iter [175/391]		data: 0.012s | batch: 0.037s	loss: 6.27487	acc: 1.01651	acc5: 5.10476
Train [cifar_base_105_squash]	epoch [0/300]	iter [180/391]		data: 0.012s | batch: 0.037s	loss: 6.15826	acc: 1.02728	acc5: 5.11913
Train [cifar_base_105_squash]	epoch [0/300]	iter [185/391]		data: 0.012s | batch: 0.037s	loss: 6.06671	acc: 1.00386	acc5: 5.10333
Train [cifar_base_105_squash]	epoch [0/300]	iter [190/391]		data: 0.012s | batch: 0.037s	loss: 5.96802	acc: 0.98577	acc5: 5.08426
Train [cifar_base_105_squash]	epoch [0/300]	iter [195/391]		data: 0.012s | batch: 0.037s	loss: 5.85504	acc: 1.00446	acc5: 5.08610
