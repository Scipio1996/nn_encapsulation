Experiment: base_101_v4_2gpu
------------ Test Options -----------------
KL_factor: 0.1
KL_manner: 1
add_cap_BN_relu: False
add_cap_dropout: False
b_init: zero
base_save_folder: result
batch_size_test: 100
batch_size_train: 100
beta1: 0.9
bigger_input: False
cap_N: 3
cap_model: v0
dataset: cifar10
debug_mode: False
depth: 14
do_squash: False
draw_hist: False
dropout_p: 0.2
experiment_name: base_101_v4_2gpu
fc_time: 0
file_name: result/base_101_v4_2gpu/opt_train_val_START_epoch_0_END_600.txt
fix_m: False
gamma: 0.1
look_into_details: False
loss_form: margin
lr: 0.0001
manual_seed: -1
max_epoch: 600
measure_time: False
momentum: 0.9
multi_crop_test: False
no_visdom: False
non_target_j: False
num_workers: 2
optim: adam
phase: train_val
port_id: 8000
pre_ch_num: 32
primary_cap_num: 32
random_seed: 5110
route_num: 3
save_epoch: 25
save_folder: result/base_101_v4_2gpu
schedule: [150, 200, 250]
scheduler: None
setting: top1
show_freq: 100
show_test_after_epoch: 100
squash_manner: paper
use_KL: False
use_cuda: True
use_instanceBN: False
use_multiple: False
w_version: v2
weight_decay: 0.0005
------------------ End --------------------
CapsNet (
  (tranfer_conv): Conv2d (3, 32, kernel_size=(9, 9), stride=(2, 2), padding=(1, 1)), weights=((32, 3, 9, 9), (32,)), parameters=7808
  (tranfer_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True), weights=((32,), (32,)), parameters=64
  (tranfer_relu): ReLU(inplace), weights=(), parameters=0
  (tranfer_conv1): Conv2d (32, 256, kernel_size=(3, 3), stride=(2, 2)), weights=((256, 32, 3, 3), (256,)), parameters=73984
  (tranfer_bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True), weights=((256,), (256,)), parameters=512
  (tranfer_relu1): ReLU(inplace), weights=(), parameters=0
  (max_pool): MaxPool2d(kernel_size=(5, 5), stride=(5, 5), dilation=(1, 1)), weights=(), parameters=0
  (cap_layer): CapLayer(
    (W): Conv2d (256, 5120, kernel_size=(1, 1), stride=(1, 1), groups=32)
  ), weights=((5120, 8, 1, 1), (5120,)), parameters=46080
)
Total param num # 0.489990 Mb

init learning rate 0.0001000000 at iter 0

[base_101_v4_2gpu]	epoch/iter [0/600][0/500] ||	Loss: 3.1185, Top1_err: 95.0000, Top5_err: 42.0000 ||	Data/batch time: 0.1904/3.3164
[base_101_v4_2gpu]	epoch/iter [0/600][100/500] ||	Loss: 0.9640, Top1_err: 81.3861, Top5_err: 33.2178 ||	Data/batch time: 0.0087/0.0690
[base_101_v4_2gpu]	epoch/iter [0/600][200/500] ||	Loss: 0.7160, Top1_err: 76.2139, Top5_err: 26.5572 ||	Data/batch time: 0.0078/0.0529
[base_101_v4_2gpu]	epoch/iter [0/600][300/500] ||	Loss: 0.6264, Top1_err: 73.1694, Top5_err: 23.5648 ||	Data/batch time: 0.0066/0.0468
[base_101_v4_2gpu]	epoch/iter [0/600][400/500] ||	Loss: 0.5791, Top1_err: 71.0549, Top5_err: 21.7481 ||	Data/batch time: 0.0068/0.0442
[base_101_v4_2gpu]	epoch/iter [0/600][499/500] ||	Loss: 0.5492, Top1_err: 69.6620, Top5_err: 20.2860 ||	Data/batch time: 0.0062/0.0423
Summary	epoch/iter [0/600] ||	TRAIN, Top1_err: 69.6620, Top5_err: 20.2860 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

model saved at result/base_101_v4_2gpu/epoch_1.pth
[base_101_v4_2gpu]	epoch/iter [1/600][0/500] ||	Loss: 0.4090, Top1_err: 59.0000, Top5_err: 14.0000 ||	Data/batch time: 0.1908/0.2358
