Experiment: baseline_mnist
------------ Train and Test Options -----------------
C_form: l2
base_save_folder: result/paper
batch_size_test: 128
batch_size_train: 128
beta1: 0.9
cap_model: v_base
coeff_dimwise: False
dataset: mnist
debug_mode: True
depth: 14
device_id: 
draw_hist: False
encapsulate_G: False
experiment_name: baseline_mnist
file_name: result/paper/baseline_mnist/opt_train_val_START_epoch_0_END_600.txt
gamma: 0.1
less_data_aug: True
loss_fac: 1.0
loss_form: CE
lr: 0.0001
manual_seed: -1
max_epoch: 600
measure_time: False
momentum: 0.9
multi_crop_test: False
net_config: default
no_bp_P_L: False
no_visdom: False
non_target_j: False
num_workers: 2
optim: rmsprop
ot_loss: False
ot_loss_fac: 1.0
phase: train_val
port_id: 9000
random_seed: 6969
remove_bias: False
s35: True
save_epoch: 1
save_folder: result/paper/baseline_mnist
schedule: [200, 300, 400]
show_freq: 1
show_test_after_epoch: 0
skip_critic: False
skip_relu: False
use_capBN: False
use_cuda: False
weight_decay: 0.0005
withCapRoute: False
------------------ End --------------------
CapNet (
  (conv1): Conv2d (1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), weights=((16, 1, 3, 3),), parameters=144
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True), weights=((16,), (16,)), parameters=32
  (relu): ReLU(inplace), weights=(), parameters=0
  (layer1): Sequential (
    (0): BasicBlock(
      (conv1): Conv2d (16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    ), weights=((16, 16, 3, 3), (16,), (16,), (16, 16, 3, 3), (16,), (16,)), parameters=4672
    (1): BasicBlock(
      (conv1): Conv2d (16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    ), weights=((16, 16, 3, 3), (16,), (16,), (16, 16, 3, 3), (16,), (16,)), parameters=4672
  ), weights=((16, 16, 3, 3), (16,), (16,), (16, 16, 3, 3), (16,), (16,), (16, 16, 3, 3), (16,), (16,), (16, 16, 3, 3), (16,), (16,)), parameters=9344
  (layer2): Sequential (
    (0): BasicBlock(
      (conv1): Conv2d (16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential(
        (0): Conv2d (16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      )
    ), weights=((32, 16, 3, 3), (32,), (32,), (32, 32, 3, 3), (32,), (32,), (32, 16, 1, 1), (32,), (32,)), parameters=14528
    (1): BasicBlock(
      (conv1): Conv2d (32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    ), weights=((32, 32, 3, 3), (32,), (32,), (32, 32, 3, 3), (32,), (32,)), parameters=18560
  ), weights=((32, 16, 3, 3), (32,), (32,), (32, 32, 3, 3), (32,), (32,), (32, 16, 1, 1), (32,), (32,), (32, 32, 3, 3), (32,), (32,), (32, 32, 3, 3), (32,), (32,)), parameters=33088
  (layer3): Sequential (
    (0): BasicBlock(
      (conv1): Conv2d (32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential(
        (0): Conv2d (32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      )
    ), weights=((64, 32, 3, 3), (64,), (64,), (64, 64, 3, 3), (64,), (64,), (64, 32, 1, 1), (64,), (64,)), parameters=57728
    (1): BasicBlock(
      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU(inplace)
      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    ), weights=((64, 64, 3, 3), (64,), (64,), (64, 64, 3, 3), (64,), (64,)), parameters=73984
  ), weights=((64, 32, 3, 3), (64,), (64,), (64, 64, 3, 3), (64,), (64,), (64, 32, 1, 1), (64,), (64,), (64, 64, 3, 3), (64,), (64,), (64, 64, 3, 3), (64,), (64,)), parameters=131712
  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0, ceil_mode=False, count_include_pad=True), weights=(), parameters=0
  (fc): Linear(in_features=64, out_features=10), weights=((10, 64), (10,)), parameters=650
)
Total param num # 0.667458 Mb

init learning rate 0.0001000000 at iter 0

[baseline_mnist]	epoch/iter [0/600][0/469] ||	Loss: 2.7636, Top1_err: 90.6250, Top5_err: 46.0938 ||	Data/batch time: 0.0247/0.6086
[baseline_mnist]	epoch/iter [0/600][1/469] ||	Loss: 2.7214, Top1_err: 91.4062, Top5_err: 44.5312 ||	Data/batch time: 0.0172/0.5829
[baseline_mnist]	epoch/iter [0/600][2/469] ||	Loss: 2.7147, Top1_err: 91.1458, Top5_err: 43.7500 ||	Data/batch time: 0.0145/0.5754
[baseline_mnist]	epoch/iter [0/600][3/469] ||	Loss: 2.6555, Top1_err: 90.0391, Top5_err: 42.1875 ||	Data/batch time: 0.0128/0.5660
[baseline_mnist]	epoch/iter [0/600][4/469] ||	Loss: 2.6119, Top1_err: 88.4375, Top5_err: 40.7812 ||	Data/batch time: 0.0119/0.5679
[baseline_mnist]	epoch/iter [0/600][5/469] ||	Loss: 2.5380, Top1_err: 84.5052, Top5_err: 38.2812 ||	Data/batch time: 0.0128/0.5694
[baseline_mnist]	epoch/iter [0/600][6/469] ||	Loss: 2.5077, Top1_err: 83.1473, Top5_err: 37.9464 ||	Data/batch time: 0.0121/0.5673
[baseline_mnist]	epoch/iter [0/600][7/469] ||	Loss: 2.4668, Top1_err: 82.1289, Top5_err: 36.5234 ||	Data/batch time: 0.0116/0.5653
[baseline_mnist]	epoch/iter [0/600][8/469] ||	Loss: 2.4216, Top1_err: 81.0764, Top5_err: 34.7222 ||	Data/batch time: 0.0112/0.5630
[baseline_mnist]	epoch/iter [0/600][9/469] ||	Loss: 2.3732, Top1_err: 79.4531, Top5_err: 33.1250 ||	Data/batch time: 0.0109/0.5656
[baseline_mnist]	epoch/iter [0/600][10/469] ||	Loss: 2.3453, Top1_err: 78.9773, Top5_err: 32.5284 ||	Data/batch time: 0.0107/0.5672
[baseline_mnist]	epoch/iter [0/600][11/469] ||	Loss: 2.3132, Top1_err: 77.9297, Top5_err: 31.5755 ||	Data/batch time: 0.0106/0.5705
[baseline_mnist]	epoch/iter [0/600][12/469] ||	Loss: 2.2902, Top1_err: 77.8846, Top5_err: 30.7091 ||	Data/batch time: 0.0104/0.5719
[baseline_mnist]	epoch/iter [0/600][13/469] ||	Loss: 2.2535, Top1_err: 76.2835, Top5_err: 29.7433 ||	Data/batch time: 0.0103/0.5676
[baseline_mnist]	epoch/iter [0/600][14/469] ||	Loss: 2.2242, Top1_err: 75.0000, Top5_err: 28.6458 ||	Data/batch time: 0.0102/0.5648
[baseline_mnist]	epoch/iter [0/600][15/469] ||	Loss: 2.1950, Top1_err: 73.8770, Top5_err: 27.5879 ||	Data/batch time: 0.0101/0.5636
[baseline_mnist]	epoch/iter [0/600][16/469] ||	Loss: 2.1677, Top1_err: 72.6562, Top5_err: 26.5165 ||	Data/batch time: 0.0100/0.5634
[baseline_mnist]	epoch/iter [0/600][17/469] ||	Loss: 2.1455, Top1_err: 71.7882, Top5_err: 25.6944 ||	Data/batch time: 0.0099/0.5629
[baseline_mnist]	epoch/iter [0/600][18/469] ||	Loss: 2.1153, Top1_err: 70.5181, Top5_err: 24.6711 ||	Data/batch time: 0.0099/0.5619
[baseline_mnist]	epoch/iter [0/600][19/469] ||	Loss: 2.0904, Top1_err: 69.6484, Top5_err: 23.7109 ||	Data/batch time: 0.0098/0.5633
