Experiment: capsule_202_v4_1_KL_only_final
------------ Training Options -------------
KL_factor: 0.1
KL_manner: 1
add_cap_dropout: False
b_init: zero
beta1: 0.9
cap_N: 3
cap_model: v4_1
dataset: cifar10
debug: True
deploy: False
do_squash: False
draw_hist: False
dropout_p: 0.2
epochs: 300
experiment_name: capsule_202_v4_1_KL_only_final
file_name: result/capsule_202_v4_1_KL_only_final/train/opt_train_start_epoch_1_end_300.txt
fix_m: False
gamma: 0.1
has_relu_in_W: False
look_into_details: False
lr: 0.0001
manual_seed: 4205
max_epoch: 300
model_cifar: capsule
momentum: 0.9
multi_crop_test: True
non_target_j: False
num_workers: 2
optim: rmsprop
phase: train
port: 4000
route_num: 2
save_epoch: 20
save_folder: result/capsule_202_v4_1_KL_only_final/train
schedule_cifar: [150, 225]
scheduler: None
send_images_to_visdom: False
show_freq: 5
show_test_after_epoch: -1
squash_manner: sigmoid
start_epoch: 1
test_batch: 128
test_only: False
train_batch: 128
use_CE_loss: False
use_KL: True
use_cuda: True
use_spread_loss: False
visdom: True
w_version: v2
weight_decay: 0.0005
------------------ End --------------------
CapsNet (
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (layer1): Sequential (
    (0): BasicBlock (
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (1): BasicBlock (
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
    (2): BasicBlock (
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer2): Sequential (
    (0): BasicBlock (
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential (
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock (
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
    (2): BasicBlock (
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (layer3): Sequential (
    (0): BasicBlock (
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (downsample): Sequential (
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (1): BasicBlock (
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
    (2): BasicBlock (
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU (inplace)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    )
  )
  (tranfer_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1))
  (tranfer_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
  (tranfer_relu): ReLU (inplace)
  (buffer): Sequential (
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU (inplace)
  )
  (basic_cap): CapLayer2 (
    (W): Conv2d(64, 4096, kernel_size=(1, 1), stride=(1, 1))
  )
  (cls_cap): CapLayer2 (
    (W): Conv2d(64, 640, kernel_size=(1, 1), stride=(1, 1))
  )
  (buffer2): Sequential (
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU (inplace)
  )
  (cap_smaller_in_share): CapLayer2 (
    (W): Conv2d(512, 32768, kernel_size=(1, 1), stride=(1, 1), groups=4)
  )
  (cap_smaller_in_out_share): CapLayer2 (
    (W): Conv2d(512, 16384, kernel_size=(1, 1), stride=(1, 1), groups=4)
  )
  (cls_smaller_in_share): CapLayer2 (
    (W): Conv2d(2048, 20480, kernel_size=(1, 1), stride=(1, 1), groups=16)
  )
  (dropout): Dropout2d (p=0.1)
  (bummer): Sequential (
    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    (1): ReLU (inplace)
  )
)

init learning rate 0.000100 at iter 0

Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[0/391]	data: 0.142s |batch: 2.451s	loss: 0.87915	(KL: 0.19126 ||normal: 0.68789)	acc: 9.37500	acc5: 50.78125
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[5/391]	data: 0.026s |batch: 1.644s	loss: 0.73241	(KL: 0.10373 ||normal: 0.62868)	acc: 10.93750	acc5: 51.82292
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[10/391]	data: 0.015s |batch: 1.570s	loss: 0.67487	(KL: 0.08000 ||normal: 0.59487)	acc: 11.93182	acc5: 56.03693
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[15/391]	data: 0.011s |batch: 1.541s	loss: 0.65099	(KL: 0.06962 ||normal: 0.58136)	acc: 12.74414	acc5: 58.64258
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[20/391]	data: 0.009s |batch: 1.522s	loss: 0.63034	(KL: 0.06249 ||normal: 0.56785)	acc: 13.83929	acc5: 60.37946
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[25/391]	data: 0.008s |batch: 1.519s	loss: 0.61252	(KL: 0.05667 ||normal: 0.55585)	acc: 15.32452	acc5: 62.77043
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[30/391]	data: 0.007s |batch: 1.512s	loss: 0.60156	(KL: 0.05230 ||normal: 0.54926)	acc: 15.90222	acc5: 64.13810
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[35/391]	data: 0.006s |batch: 1.507s	loss: 0.59070	(KL: 0.04891 ||normal: 0.54179)	acc: 16.77517	acc5: 65.58160
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[40/391]	data: 0.006s |batch: 1.502s	loss: 0.58126	(KL: 0.04591 ||normal: 0.53535)	acc: 17.45427	acc5: 66.97790
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[45/391]	data: 0.006s |batch: 1.501s	loss: 0.57190	(KL: 0.04335 ||normal: 0.52855)	acc: 18.69905	acc5: 68.17255
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[50/391]	data: 0.005s |batch: 1.498s	loss: 0.56392	(KL: 0.04098 ||normal: 0.52293)	acc: 19.59252	acc5: 69.24020
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[55/391]	data: 0.005s |batch: 1.495s	loss: 0.55489	(KL: 0.03882 ||normal: 0.51607)	acc: 20.64732	acc5: 70.20089
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[60/391]	data: 0.005s |batch: 1.493s	loss: 0.54799	(KL: 0.03679 ||normal: 0.51119)	acc: 21.28586	acc5: 71.14498
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[65/391]	data: 0.005s |batch: 1.493s	loss: 0.54154	(KL: 0.03502 ||normal: 0.50653)	acc: 22.05256	acc5: 71.99337
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[70/391]	data: 0.005s |batch: 1.491s	loss: 0.53567	(KL: 0.03349 ||normal: 0.50218)	acc: 22.73327	acc5: 72.79930
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[75/391]	data: 0.004s |batch: 1.490s	loss: 0.53071	(KL: 0.03213 ||normal: 0.49858)	acc: 23.22163	acc5: 73.40666
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[80/391]	data: 0.004s |batch: 1.490s	loss: 0.52543	(KL: 0.03094 ||normal: 0.49450)	acc: 23.96798	acc5: 74.01620
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[85/391]	data: 0.004s |batch: 1.491s	loss: 0.52166	(KL: 0.02983 ||normal: 0.49183)	acc: 24.37318	acc5: 74.49128
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[90/391]	data: 0.004s |batch: 1.491s	loss: 0.51697	(KL: 0.02885 ||normal: 0.48812)	acc: 24.96566	acc5: 74.93990
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[95/391]	data: 0.004s |batch: 1.491s	loss: 0.51399	(KL: 0.02797 ||normal: 0.48602)	acc: 25.26042	acc5: 75.28483
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[100/391]	data: 0.004s |batch: 1.490s	loss: 0.51170	(KL: 0.02717 ||normal: 0.48453)	acc: 25.51825	acc5: 75.57240
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[105/391]	data: 0.004s |batch: 1.490s	loss: 0.50844	(KL: 0.02641 ||normal: 0.48203)	acc: 25.84021	acc5: 75.99499
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[110/391]	data: 0.004s |batch: 1.489s	loss: 0.50545	(KL: 0.02567 ||normal: 0.47978)	acc: 26.23170	acc5: 76.42877
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[115/391]	data: 0.004s |batch: 1.489s	loss: 0.50256	(KL: 0.02501 ||normal: 0.47754)	acc: 26.57597	acc5: 76.79822
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[120/391]	data: 0.004s |batch: 1.489s	loss: 0.50042	(KL: 0.02438 ||normal: 0.47604)	acc: 26.77557	acc5: 77.00801
Train [capsule_202_v4_1_KL_only_final]	epoch [0/300]|[125/391]	data: 0.004s |batch: 1.488s	loss: 0.49816	(KL: 0.02380 ||normal: 0.47436)	acc: 26.97173	acc5: 77.28175
