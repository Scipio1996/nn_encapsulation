Experiment: base_101_EM_v3
------------ Train and Test Options -----------------
E_step_norm: False
add_cap_BN_relu: False
add_cap_dropout: False
b_init: zero
base_save_folder: result
batch_size_test: 128
batch_size_train: 128
beta1: 0.9
cap_model: v0
comp_cap: False
dataset: cifar10
debug_mode: False
device_id: 0
draw_hist: False
dropout_p: 0.2
experiment_name: base_101_EM_v3
file_name: result/base_101_EM_v3/opt_train_val_START_epoch_0_END_600.txt
gamma: 0.1
less_data_aug: False
loss_form: margin
lr: 0.0001
manual_seed: -1
max_epoch: 600
measure_time: False
momentum: 0.9
multi_crop_test: False
no_visdom: False
non_target_j: False
num_workers: 2
optim: adam
phase: train_val
port_id: 8000
pre_ch_num: 256
primary_cap_num: 32
random_seed: 9621
route: EM
route_num: 3
s35: False
save_epoch: 25
save_folder: result/base_101_EM_v3
schedule: [200, 300, 400]
show_freq: 100
show_test_after_epoch: 100
squash_manner: paper
use_cuda: True
use_instanceBN: False
weight_decay: 0.0005
------------------ End --------------------
DataParallel (
  (module): CapNet(
    (tranfer_conv): Conv2d (3, 256, kernel_size=(9, 9), stride=(2, 2), padding=(1, 1))
    (tranfer_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (tranfer_relu): ReLU(inplace)
    (tranfer_conv1): Conv2d (256, 256, kernel_size=(3, 3), stride=(2, 2))
    (tranfer_bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (tranfer_relu1): ReLU(inplace)
    (generate_activate): Sequential(
      (0): Conv2d (256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (2): ReLU()
    )
    (cap_layer): CapLayer(
      (W): Conv2d (256, 5120, kernel_size=(1, 1), stride=(1, 1), groups=32)
    )
  ), weights=((256, 3, 9, 9), (256,), (256,), (256,), (256, 256, 3, 3), (256,), (256,), (256,), (32, 256, 3, 3), (32,), (32,), (32,), (10,), (10,), (5120, 8, 1, 1), (5120,)), parameters=773492
)
Total param num # 2.950638 Mb

init learning rate 0.0001000000 at iter 0

[base_101_EM_v3]	epoch/iter [0/600][0/391] ||	Loss: 17.1071, Top1_err: 90.6250, Top5_err: 49.2188 ||	Data/batch time: 0.2907/6.0596
[base_101_EM_v3]	epoch/iter [0/600][100/391] ||	Loss: 9.8411, Top1_err: 88.7299, Top5_err: 50.2088 ||	Data/batch time: 0.0035/0.2154
[base_101_EM_v3]	epoch/iter [0/600][200/391] ||	Loss: 7.5152, Top1_err: 88.9770, Top5_err: 50.7618 ||	Data/batch time: 0.0019/0.1868
[base_101_EM_v3]	epoch/iter [0/600][300/391] ||	Loss: 5.7503, Top1_err: 88.8185, Top5_err: 50.7397 ||	Data/batch time: 0.0014/0.1819
[base_101_EM_v3]	epoch/iter [0/600][390/391] ||	Loss: 4.6621, Top1_err: 88.2120, Top5_err: 50.0040 ||	Data/batch time: 0.0011/0.1807
Summary	epoch/iter [0/600] ||	TRAIN, Top1_err: 88.2120, Top5_err: 50.0040 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

model saved at result/base_101_EM_v3/epoch_1.pth
[base_101_EM_v3]	epoch/iter [1/600][0/391] ||	Loss: 0.7686, Top1_err: 85.1562, Top5_err: 43.7500 ||	Data/batch time: 0.1054/0.2467
[base_101_EM_v3]	epoch/iter [1/600][100/391] ||	Loss: 0.6589, Top1_err: 82.8976, Top5_err: 40.6791 ||	Data/batch time: 0.0016/0.1778
[base_101_EM_v3]	epoch/iter [1/600][200/391] ||	Loss: 0.6059, Top1_err: 81.5260, Top5_err: 37.1035 ||	Data/batch time: 0.0010/0.1762
[base_101_EM_v3]	epoch/iter [1/600][300/391] ||	Loss: 0.5729, Top1_err: 79.6044, Top5_err: 33.9961 ||	Data/batch time: 0.0008/0.1755
[base_101_EM_v3]	epoch/iter [1/600][390/391] ||	Loss: 0.5531, Top1_err: 77.9060, Top5_err: 31.5700 ||	Data/batch time: 0.0008/0.1753
Summary	epoch/iter [1/600] ||	TRAIN, Top1_err: 77.9060, Top5_err: 31.5700 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

[base_101_EM_v3]	epoch/iter [2/600][0/391] ||	Loss: 0.4692, Top1_err: 65.6250, Top5_err: 22.6562 ||	Data/batch time: 0.1614/0.3018
[base_101_EM_v3]	epoch/iter [2/600][100/391] ||	Loss: 0.4751, Top1_err: 71.2639, Top5_err: 21.8054 ||	Data/batch time: 0.0025/0.1767
[base_101_EM_v3]	epoch/iter [2/600][200/391] ||	Loss: 0.4688, Top1_err: 70.0871, Top5_err: 20.8178 ||	Data/batch time: 0.0015/0.1758
