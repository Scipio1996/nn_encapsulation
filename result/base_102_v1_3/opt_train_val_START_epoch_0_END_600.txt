Experiment: base_102_v1_3
------------ Test Options -----------------
KL_factor: 0.1
KL_manner: 1
add_cap_BN_relu: False
add_cap_dropout: False
b_init: zero
base_save_folder: result
batch_size_test: 128
batch_size_train: 128
beta1: 0.9
bigger_input: False
cap_N: 4
cap_model: v1_3
comp_cap: False
connect_detail: all
dataset: cifar10
debug_mode: True
depth: 14
device_id: 
draw_hist: False
dropout_p: 0.2
experiment_name: base_102_v1_3
fc_manner: default
file_name: result/base_102_v1_3/opt_train_val_START_epoch_0_END_600.txt
fix_m: False
gamma: 0.1
less_data_aug: True
loss_form: margin
lr: 0.0001
manual_seed: -1
max_epoch: 600
measure_time: False
momentum: 0.9
multi_crop_test: False
no_visdom: False
non_target_j: False
num_workers: 2
optim: adam
phase: train_val
port_id: 9000
pre_ch_num: 256
primary_cap_num: 32
random_seed: 2873
route_num: 3
s35: True
save_epoch: 1
save_folder: result/base_102_v1_3
schedule: [200, 300, 400]
setting: top1
show_freq: 1
show_test_after_epoch: 0
squash_manner: paper
use_KL: False
use_cuda: False
use_instanceBN: False
use_multiple: False
weight_decay: 0.0005
------------------ End --------------------
CapsNet (
  (layer1): Sequential (
    (0): Conv2d (3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), weights=((32, 3, 3, 3), (32,)), parameters=896
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True), weights=((32,), (32,)), parameters=64
    (2): ReLU(inplace), weights=(), parameters=0
  ), weights=((32, 3, 3, 3), (32,), (32,), (32,)), parameters=960
  (cap1_conv): CapConv(
    (conv_adjust_blob_shape): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (sub_layer): Sequential(
      (0): Conv2d (32, 64, kernel_size=(1, 1), stride=(1, 1), groups=32)
    )
    (last_relu): ReLU()
    (last_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (last_squash): conv_squash(num_shared=32)
  ), weights=((64, 32, 3, 3), (64,), (64, 1, 1, 1), (64,), (64,), (64,)), parameters=18752
  (cap1_conv_sub): CapConv(
    (sub_layer): Sequential(
      (0): Conv2d (64, 64, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (2): ReLU(inplace)
      (3): conv_squash(num_shared=32)
      (4): Conv2d (64, 64, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (6): ReLU(inplace)
      (7): conv_squash(num_shared=32)
      (8): Conv2d (64, 64, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (10): ReLU(inplace)
      (11): conv_squash(num_shared=32)
      (12): Conv2d (64, 64, kernel_size=(1, 1), stride=(1, 1), groups=32)
    )
    (last_relu): ReLU()
    (last_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (last_squash): conv_squash(num_shared=32)
  ), weights=((64, 2, 1, 1), (64,), (64,), (64,), (64, 2, 1, 1), (64,), (64,), (64,), (64, 2, 1, 1), (64,), (64,), (64,), (64, 2, 1, 1), (64,), (64,), (64,)), parameters=1280
  (cap2_conv): CapConv(
    (conv_adjust_blob_shape): Conv2d (64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (sub_layer): Sequential(
      (0): Conv2d (64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)
    )
    (last_relu): ReLU()
    (last_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    (last_squash): conv_squash(num_shared=32)
  ), weights=((128, 64, 3, 3), (128,), (128, 2, 3, 3), (128,), (128,), (128,)), parameters=76544
  (cap2_conv_sub): CapConv(
    (sub_layer): Sequential(
      (0): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (2): ReLU(inplace)
      (3): conv_squash(num_shared=32)
      (4): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (6): ReLU(inplace)
      (7): conv_squash(num_shared=32)
      (8): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (10): ReLU(inplace)
      (11): conv_squash(num_shared=32)
      (12): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1), groups=32)
    )
    (last_relu): ReLU()
    (last_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
    (last_squash): conv_squash(num_shared=32)
  ), weights=((128, 4, 1, 1), (128,), (128,), (128,), (128, 4, 1, 1), (128,), (128,), (128,), (128, 4, 1, 1), (128,), (128,), (128,), (128, 4, 1, 1), (128,), (128,), (128,)), parameters=3584
  (cap3_conv): CapConv(
    (conv_adjust_blob_shape): Conv2d (128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (sub_layer): Sequential(
      (0): Conv2d (128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)
    )
    (last_relu): ReLU()
    (last_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (last_squash): conv_squash(num_shared=32)
  ), weights=((256, 128, 3, 3), (256,), (256, 4, 3, 3), (256,), (256,), (256,)), parameters=305152
  (cap3_conv_sub): CapConv(
    (sub_layer): Sequential(
      (0): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (2): ReLU(inplace)
      (3): conv_squash(num_shared=32)
      (4): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (6): ReLU(inplace)
      (7): conv_squash(num_shared=32)
      (8): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
      (10): ReLU(inplace)
      (11): conv_squash(num_shared=32)
      (12): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32)
    )
    (last_relu): ReLU()
    (last_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (last_squash): conv_squash(num_shared=32)
  ), weights=((256, 8, 1, 1), (256,), (256,), (256,), (256, 8, 1, 1), (256,), (256,), (256,), (256, 8, 1, 1), (256,), (256,), (256,), (256, 8, 1, 1), (256,), (256,), (256,)), parameters=11264
  (cap4_conv): CapConv(
    (conv_adjust_blob_shape): Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (sub_layer): Sequential(
      (0): Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)
    )
    (last_relu): ReLU()
    (last_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
    (last_squash): conv_squash(num_shared=32)
  ), weights=((512, 256, 3, 3), (512,), (512, 8, 3, 3), (512,), (512,), (512,)), parameters=1218560
  (cap4_conv_sub): CapConv(
    (sub_layer): Sequential(
      (0): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (2): ReLU(inplace)
      (3): conv_squash(num_shared=32)
      (4): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (6): ReLU(inplace)
      (7): conv_squash(num_shared=32)
      (8): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32)
      (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
      (10): ReLU(inplace)
      (11): conv_squash(num_shared=32)
      (12): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32)
    )
    (last_relu): ReLU()
    (last_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
    (last_squash): conv_squash(num_shared=32)
  ), weights=((512, 16, 1, 1), (512,), (512,), (512,), (512, 16, 1, 1), (512,), (512,), (512,), (512, 16, 1, 1), (512,), (512,), (512,), (512, 16, 1, 1), (512,), (512,), (512,)), parameters=38912
  (final_cls): CapFC(in_cap_num=512, out_cap_num=10, cap_dim=16, fc_manner=default), weights=((16, 512, 10),), parameters=81920
)
Total param num # 6.702148 Mb

init learning rate 0.0001000000 at iter 0

[base_102_v1_3]	epoch/iter [0/600][0/391] ||	Loss: 0.5842, Top1_err: 92.1875, Top5_err: 42.1875 ||	Data/batch time: 0.0506/12.2103
[base_102_v1_3]	epoch/iter [0/600][1/391] ||	Loss: 0.5828, Top1_err: 91.7969, Top5_err: 43.3594 ||	Data/batch time: 0.0392/12.8100
[base_102_v1_3]	epoch/iter [0/600][2/391] ||	Loss: 0.5777, Top1_err: 89.3229, Top5_err: 43.4896 ||	Data/batch time: 0.0343/13.1197
[base_102_v1_3]	epoch/iter [0/600][3/391] ||	Loss: 0.5768, Top1_err: 89.4531, Top5_err: 45.1172 ||	Data/batch time: 0.0270/12.8199
[base_102_v1_3]	epoch/iter [0/600][4/391] ||	Loss: 0.5751, Top1_err: 89.2188, Top5_err: 45.9375 ||	Data/batch time: 0.0231/12.8305
[base_102_v1_3]	epoch/iter [0/600][5/391] ||	Loss: 0.5714, Top1_err: 89.0625, Top5_err: 45.1823 ||	Data/batch time: 0.0202/12.7946
[base_102_v1_3]	epoch/iter [0/600][6/391] ||	Loss: 0.5684, Top1_err: 88.5045, Top5_err: 44.1964 ||	Data/batch time: 0.0185/12.8011
[base_102_v1_3]	epoch/iter [0/600][7/391] ||	Loss: 0.5663, Top1_err: 88.4766, Top5_err: 43.9453 ||	Data/batch time: 0.0171/12.9288
[base_102_v1_3]	epoch/iter [0/600][8/391] ||	Loss: 0.5647, Top1_err: 88.6285, Top5_err: 44.5312 ||	Data/batch time: 0.0166/13.0122
[base_102_v1_3]	epoch/iter [0/600][9/391] ||	Loss: 0.5610, Top1_err: 88.4375, Top5_err: 43.4375 ||	Data/batch time: 0.0166/13.0004
[base_102_v1_3]	epoch/iter [0/600][10/391] ||	Loss: 0.5582, Top1_err: 87.9261, Top5_err: 43.1108 ||	Data/batch time: 0.0156/13.0432
[base_102_v1_3]	epoch/iter [0/600][11/391] ||	Loss: 0.5558, Top1_err: 87.5000, Top5_err: 43.1641 ||	Data/batch time: 0.0149/13.0670
[base_102_v1_3]	epoch/iter [0/600][12/391] ||	Loss: 0.5515, Top1_err: 86.7188, Top5_err: 42.0673 ||	Data/batch time: 0.0143/13.0825
[base_102_v1_3]	epoch/iter [0/600][13/391] ||	Loss: 0.5489, Top1_err: 86.7746, Top5_err: 41.4062 ||	Data/batch time: 0.0137/13.1134
[base_102_v1_3]	epoch/iter [0/600][14/391] ||	Loss: 0.5465, Top1_err: 86.1979, Top5_err: 41.0938 ||	Data/batch time: 0.0132/13.0702
[base_102_v1_3]	epoch/iter [0/600][15/391] ||	Loss: 0.5441, Top1_err: 85.6445, Top5_err: 40.7227 ||	Data/batch time: 0.0130/13.0454
