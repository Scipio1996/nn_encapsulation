Experiment: cap_102_v4_1
------------ Train and Test Options -----------------
C_form: l2
base_save_folder: result
batch_size_test: 128
batch_size_train: 128
beta1: 0.9
cap_N: 4
cap_model: v2
coeff_dimwise: False
connect_detail: all
dataset: cifar10
debug_mode: True
device_id: 
draw_hist: False
encapsulate_G: False
experiment_name: cap_102_v4_1
fc_manner: default
file_name: result/cap_102_v4_1/opt_train_val_START_epoch_0_END_600.txt
gamma: 0.1
layerwise: True
less_data_aug: True
loss_fac: 1.0
loss_form: margin
lr: 0.0001
manner: 0
manual_seed: -1
max_epoch: 600
measure_time: False
momentum: 0.9
multi_crop_test: False
net_config: default
no_bp_P_L: False
no_visdom: False
non_target_j: False
num_workers: 2
optim: adam
ot_loss: False
ot_loss_fac: 1.0
phase: train_val
port_id: 8000
random_seed: 1280
remove_bias: False
s35: False
save_epoch: 1
save_folder: result/cap_102_v4_1
schedule: [200, 300, 400]
show_freq: 1
show_test_after_epoch: 0
skip_critic: False
skip_relu: False
use_capBN: True
use_cuda: True
weight_decay: 0.0005
wider: True
withCapRoute: False
------------------ End --------------------
CapNet (
  (module0): Sequential (
    (0): Conv2d (3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), weights=((32, 3, 3, 3), (32,)), parameters=896
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True), weights=((32,), (32,)), parameters=64
    (2): ReLU(inplace), weights=(), parameters=0
  ), weights=((32, 3, 3, 3), (32,), (32,), (32,)), parameters=960
  (module1): CapConv2(
    (main_conv): CapConv(
      (block): ModuleList(
        (0): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32)
            (1): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
            (2): Conv2d (32, 64, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU(inplace)
        (3): conv_squash(num_shared=32)
      )
      (conv_adjust_blob_shape): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (sub_conv): CapConv(
      (block): ModuleList(
        (0): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (64, 64, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU(inplace)
        (3): conv_squash(num_shared=32)
        (4): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (64, 64, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (5): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (6): ReLU(inplace)
        (7): conv_squash(num_shared=32)
        (8): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (64, 64, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (9): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (10): ReLU(inplace)
        (11): conv_squash(num_shared=32)
        (12): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (64, 64, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (13): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (14): ReLU(inplace)
        (15): conv_squash(num_shared=32)
      )
    )
  ), weights=((64, 1, 5, 5), (64,), (64, 1, 3, 3), (64,), (64, 1, 1, 1), (64,), (32,), (32,), (64, 32, 3, 3), (64,), (64, 2, 1, 1), (64,), (32,), (32,), (64, 2, 1, 1), (64,), (32,), (32,), (64, 2, 1, 1), (64,), (32,), (32,), (64, 2, 1, 1), (64,), (32,), (32,)), parameters=22016
  (module2): CapConv2(
    (main_conv): CapConv(
      (block): ModuleList(
        (0): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=32)
            (1): Conv2d (64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)
            (2): Conv2d (64, 128, kernel_size=(1, 1), stride=(2, 2), groups=32)
          )
        )
        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU(inplace)
        (3): conv_squash(num_shared=32)
      )
      (conv_adjust_blob_shape): Conv2d (64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (sub_conv): CapConv(
      (block): ModuleList(
        (0): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU(inplace)
        (3): conv_squash(num_shared=32)
        (4): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (5): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (6): ReLU(inplace)
        (7): conv_squash(num_shared=32)
        (8): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (9): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (10): ReLU(inplace)
        (11): conv_squash(num_shared=32)
        (12): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (128, 128, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (13): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (14): ReLU(inplace)
        (15): conv_squash(num_shared=32)
      )
    )
  ), weights=((128, 2, 5, 5), (128,), (128, 2, 3, 3), (128,), (128, 2, 1, 1), (128,), (32,), (32,), (128, 64, 3, 3), (128,), (128, 4, 1, 1), (128,), (32,), (32,), (128, 4, 1, 1), (128,), (32,), (32,), (128, 4, 1, 1), (128,), (32,), (32,), (128, 4, 1, 1), (128,), (32,), (32,)), parameters=86080
  (module3): CapConv2(
    (main_conv): CapConv(
      (block): ModuleList(
        (0): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=32)
            (1): Conv2d (128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)
            (2): Conv2d (128, 256, kernel_size=(1, 1), stride=(2, 2), groups=32)
          )
        )
        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU(inplace)
        (3): conv_squash(num_shared=32)
      )
      (conv_adjust_blob_shape): Conv2d (128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (sub_conv): CapConv(
      (block): ModuleList(
        (0): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU(inplace)
        (3): conv_squash(num_shared=32)
        (4): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (5): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (6): ReLU(inplace)
        (7): conv_squash(num_shared=32)
        (8): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (9): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (10): ReLU(inplace)
        (11): conv_squash(num_shared=32)
        (12): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (13): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (14): ReLU(inplace)
        (15): conv_squash(num_shared=32)
      )
    )
  ), weights=((256, 4, 5, 5), (256,), (256, 4, 3, 3), (256,), (256, 4, 1, 1), (256,), (32,), (32,), (256, 128, 3, 3), (256,), (256, 8, 1, 1), (256,), (32,), (32,), (256, 8, 1, 1), (256,), (32,), (32,), (256, 8, 1, 1), (256,), (32,), (32,), (256, 8, 1, 1), (256,), (32,), (32,)), parameters=341312
  (module4): CapConv2(
    (main_conv): CapConv(
      (block): ModuleList(
        (0): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=32)
            (1): Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)
            (2): Conv2d (256, 512, kernel_size=(1, 1), stride=(2, 2), groups=32)
          )
        )
        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU(inplace)
        (3): conv_squash(num_shared=32)
      )
      (conv_adjust_blob_shape): Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
    (sub_conv): CapConv(
      (block): ModuleList(
        (0): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (2): ReLU(inplace)
        (3): conv_squash(num_shared=32)
        (4): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (5): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (6): ReLU(inplace)
        (7): conv_squash(num_shared=32)
        (8): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (9): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (10): ReLU(inplace)
        (11): conv_squash(num_shared=32)
        (12): basic_conv(
          (conv): ModuleList(
            (0): Conv2d (512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32)
          )
        )
        (13): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True)
        (14): ReLU(inplace)
        (15): conv_squash(num_shared=32)
      )
    )
  ), weights=((512, 8, 5, 5), (512,), (512, 8, 3, 3), (512,), (512, 8, 1, 1), (512,), (32,), (32,), (512, 256, 3, 3), (512,), (512, 16, 1, 1), (512,), (32,), (32,), (512, 16, 1, 1), (512,), (32,), (32,), (512, 16, 1, 1), (512,), (32,), (32,), (512, 16, 1, 1), (512,), (32,), (32,)), parameters=1360192
  (final_cls): CapFC(in_cap_num=512, out_cap_num=10, cap_dim=16, fc_manner=default), weights=((16, 512, 10),), parameters=81920
)
Total param num # 7.219238 Mb

init learning rate 0.0001000000 at iter 0

[cap_102_v4_1]	epoch/iter [0/600][0/391] ||	Loss: 0.6243, Top1_err: 93.7500, Top5_err: 45.3125 ||	Data/batch time: 0.0480/1.3169
[cap_102_v4_1]	epoch/iter [0/600][1/391] ||	Loss: 0.6212, Top1_err: 92.9688, Top5_err: 45.7031 ||	Data/batch time: 0.0342/0.8480
[cap_102_v4_1]	epoch/iter [0/600][2/391] ||	Loss: 0.6167, Top1_err: 91.6667, Top5_err: 47.6562 ||	Data/batch time: 0.0251/0.6995
[cap_102_v4_1]	epoch/iter [0/600][3/391] ||	Loss: 0.6117, Top1_err: 90.0391, Top5_err: 48.8281 ||	Data/batch time: 0.0205/0.6252
[cap_102_v4_1]	epoch/iter [0/600][4/391] ||	Loss: 0.6053, Top1_err: 89.0625, Top5_err: 47.3438 ||	Data/batch time: 0.0196/0.5821
[cap_102_v4_1]	epoch/iter [0/600][5/391] ||	Loss: 0.6015, Top1_err: 88.8021, Top5_err: 48.5677 ||	Data/batch time: 0.0175/0.5522
[cap_102_v4_1]	epoch/iter [0/600][6/391] ||	Loss: 0.5968, Top1_err: 88.2812, Top5_err: 49.2188 ||	Data/batch time: 0.0462/0.5565
[cap_102_v4_1]	epoch/iter [0/600][7/391] ||	Loss: 0.5939, Top1_err: 88.7695, Top5_err: 50.3906 ||	Data/batch time: 0.0417/0.5386
[cap_102_v4_1]	epoch/iter [0/600][8/391] ||	Loss: 0.5907, Top1_err: 89.2361, Top5_err: 50.7812 ||	Data/batch time: 0.0385/0.5234
[cap_102_v4_1]	epoch/iter [0/600][9/391] ||	Loss: 0.5869, Top1_err: 89.2188, Top5_err: 50.7031 ||	Data/batch time: 0.0550/0.5276
[cap_102_v4_1]	epoch/iter [0/600][10/391] ||	Loss: 0.5834, Top1_err: 89.3466, Top5_err: 50.4972 ||	Data/batch time: 0.0518/0.5169
[cap_102_v4_1]	epoch/iter [0/600][11/391] ||	Loss: 0.5799, Top1_err: 89.2578, Top5_err: 50.3906 ||	Data/batch time: 0.0482/0.5076
[cap_102_v4_1]	epoch/iter [0/600][12/391] ||	Loss: 0.5769, Top1_err: 89.3630, Top5_err: 50.8413 ||	Data/batch time: 0.0450/0.4995
[cap_102_v4_1]	epoch/iter [0/600][13/391] ||	Loss: 0.5740, Top1_err: 89.5089, Top5_err: 51.1161 ||	Data/batch time: 0.0423/0.4926
[cap_102_v4_1]	epoch/iter [0/600][14/391] ||	Loss: 0.5710, Top1_err: 89.3229, Top5_err: 50.9375 ||	Data/batch time: 0.0400/0.4870
[cap_102_v4_1]	epoch/iter [0/600][15/391] ||	Loss: 0.5682, Top1_err: 89.2090, Top5_err: 50.7324 ||	Data/batch time: 0.0387/0.4817
[cap_102_v4_1]	epoch/iter [0/600][16/391] ||	Loss: 0.5661, Top1_err: 89.4761, Top5_err: 51.1029 ||	Data/batch time: 0.0374/0.4771
[cap_102_v4_1]	epoch/iter [0/600][17/391] ||	Loss: 0.5638, Top1_err: 89.4965, Top5_err: 51.1285 ||	Data/batch time: 0.0358/0.4730
[cap_102_v4_1]	epoch/iter [0/600][18/391] ||	Loss: 0.5618, Top1_err: 89.2270, Top5_err: 51.1102 ||	Data/batch time: 0.0344/0.4707
[cap_102_v4_1]	epoch/iter [0/600][19/391] ||	Loss: 0.5601, Top1_err: 89.3750, Top5_err: 50.8984 ||	Data/batch time: 0.0330/0.4686
[cap_102_v4_1]	epoch/iter [0/600][20/391] ||	Loss: 0.5584, Top1_err: 89.5089, Top5_err: 51.1161 ||	Data/batch time: 0.0319/0.4664
[cap_102_v4_1]	epoch/iter [0/600][21/391] ||	Loss: 0.5570, Top1_err: 89.5952, Top5_err: 51.1719 ||	Data/batch time: 0.0308/0.4635
[cap_102_v4_1]	epoch/iter [0/600][22/391] ||	Loss: 0.5556, Top1_err: 89.5720, Top5_err: 51.1549 ||	Data/batch time: 0.0298/0.4610
[cap_102_v4_1]	epoch/iter [0/600][23/391] ||	Loss: 0.5543, Top1_err: 89.6159, Top5_err: 51.2370 ||	Data/batch time: 0.0289/0.4586
[cap_102_v4_1]	epoch/iter [0/600][24/391] ||	Loss: 0.5532, Top1_err: 89.7188, Top5_err: 51.2500 ||	Data/batch time: 0.0285/0.4564
[cap_102_v4_1]	epoch/iter [0/600][25/391] ||	Loss: 0.5521, Top1_err: 89.7536, Top5_err: 51.3221 ||	Data/batch time: 0.0277/0.4544
[cap_102_v4_1]	epoch/iter [0/600][26/391] ||	Loss: 0.5511, Top1_err: 89.6412, Top5_err: 51.3021 ||	Data/batch time: 0.0270/0.4533
[cap_102_v4_1]	epoch/iter [0/600][27/391] ||	Loss: 0.5503, Top1_err: 89.6763, Top5_err: 51.3951 ||	Data/batch time: 0.0275/0.4525
[cap_102_v4_1]	epoch/iter [0/600][28/391] ||	Loss: 0.5495, Top1_err: 89.7091, Top5_err: 51.3200 ||	Data/batch time: 0.0268/0.4511
[cap_102_v4_1]	epoch/iter [0/600][29/391] ||	Loss: 0.5487, Top1_err: 89.6615, Top5_err: 51.2240 ||	Data/batch time: 0.0261/0.4501
[cap_102_v4_1]	epoch/iter [0/600][30/391] ||	Loss: 0.5481, Top1_err: 89.8438, Top5_err: 51.7389 ||	Data/batch time: 0.0256/0.4492
[cap_102_v4_1]	epoch/iter [0/600][31/391] ||	Loss: 0.5475, Top1_err: 89.8438, Top5_err: 51.6113 ||	Data/batch time: 0.0252/0.4478
[cap_102_v4_1]	epoch/iter [0/600][32/391] ||	Loss: 0.5468, Top1_err: 89.7491, Top5_err: 51.5862 ||	Data/batch time: 0.0247/0.4469
[cap_102_v4_1]	epoch/iter [0/600][33/391] ||	Loss: 0.5462, Top1_err: 89.6829, Top5_err: 51.4936 ||	Data/batch time: 0.0242/0.4459
[cap_102_v4_1]	epoch/iter [0/600][34/391] ||	Loss: 0.5456, Top1_err: 89.6875, Top5_err: 51.2500 ||	Data/batch time: 0.0242/0.4447
[cap_102_v4_1]	epoch/iter [0/600][35/391] ||	Loss: 0.5451, Top1_err: 89.6701, Top5_err: 51.3672 ||	Data/batch time: 0.0240/0.4440
[cap_102_v4_1]	epoch/iter [0/600][36/391] ||	Loss: 0.5446, Top1_err: 89.7593, Top5_err: 51.4569 ||	Data/batch time: 0.0236/0.4430
[cap_102_v4_1]	epoch/iter [0/600][37/391] ||	Loss: 0.5442, Top1_err: 89.9054, Top5_err: 51.5419 ||	Data/batch time: 0.0232/0.4422
[cap_102_v4_1]	epoch/iter [0/600][38/391] ||	Loss: 0.5436, Top1_err: 89.8638, Top5_err: 51.4223 ||	Data/batch time: 0.0232/0.4416
[cap_102_v4_1]	epoch/iter [0/600][39/391] ||	Loss: 0.5431, Top1_err: 89.7461, Top5_err: 51.5039 ||	Data/batch time: 0.0229/0.4406
[cap_102_v4_1]	epoch/iter [0/600][40/391] ||	Loss: 0.5427, Top1_err: 89.7294, Top5_err: 51.3338 ||	Data/batch time: 0.0225/0.4398
