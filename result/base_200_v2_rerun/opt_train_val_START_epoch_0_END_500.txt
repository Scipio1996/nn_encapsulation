Experiment: base_200_v2_rerun
------------ Test Options -----------------
KL_factor: 0.1
KL_manner: 1
add_cap_BN_relu: False
add_cap_dropout: False
b_init: zero
base_save_folder: result
batch_size_test: 256
batch_size_train: 512
beta1: 0.9
bigger_input: True
cap_N: 3
cap_model: v_base
dataset: tiny_imagenet
debug_mode: False
depth: 14
do_squash: False
draw_hist: False
dropout_p: 0.2
experiment_name: base_200_v2_rerun
fc_time: 0
file_name: result/base_200_v2_rerun/opt_train_val_START_epoch_0_END_500.txt
fix_m: False
gamma: 0.1
look_into_details: False
loss_form: CE
lr: 0.0001
manual_seed: -1
max_epoch: 500
momentum: 0.9
multi_crop_test: False
no_visdom: False
non_target_j: False
num_workers: 32
optim: rmsprop
phase: train_val
port_id: 8000
pre_ch_num: 32
primary_cap_num: 32
random_seed: 2099
route_num: 3
save_epoch: 25
save_folder: result/base_200_v2_rerun
schedule: [200, 300, 400]
scheduler: None
setting: top1
show_freq: 100
show_test_after_epoch: 100
squash_manner: paper
use_KL: False
use_cuda: True
use_instanceBN: False
use_multiple: False
w_version: v2
weight_decay: 0.0005
------------------ End --------------------
DataParallel (
  (module): CapsNet (
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (layer1): Sequential (
      (0): BasicBlock (
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      )
      (1): BasicBlock (
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (layer2): Sequential (
      (0): BasicBlock (
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (downsample): Sequential (
          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        )
      )
      (1): BasicBlock (
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (layer3): Sequential (
      (0): BasicBlock (
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (downsample): Sequential (
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        )
      )
      (1): BasicBlock (
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      )
    )
    (avgpool): AvgPool2d (size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)
    (fc): Linear (1024 -> 200)
  ), weights=((16, 3, 3, 3), (16,), (16,), (16, 16, 3, 3), (16,), (16,), (16, 16, 3, 3), (16,), (16,), (16, 16, 3, 3), (16,), (16,), (16, 16, 3, 3), (16,), (16,), (32, 16, 3, 3), (32,), (32,), (32, 32, 3, 3), (32,), (32,), (32, 16, 1, 1), (32,), (32,), (32, 32, 3, 3), (32,), (32,), (32, 32, 3, 3), (32,), (32,), (64, 32, 3, 3), (64,), (64,), (64, 64, 3, 3), (64,), (64,), (64, 32, 1, 1), (64,), (64,), (64, 64, 3, 3), (64,), (64,), (64, 64, 3, 3), (64,), (64,), (200, 1024), (200,)), parameters=379608
)
Total param num # 1.448090 Mb

init learning rate 0.0001000000 at iter 0

[base_200_v2_rerun]	epoch/iter [0/500][0/500] ||	Loss: 6.6468, Top1_err: 99.4141, Top5_err: 97.4609 ||	Data/batch time: 20.9342/32.9440
[base_200_v2_rerun]	epoch/iter [0/500][100/500] ||	Loss: 5.1094, Top1_err: 97.2424, Top5_err: 90.0004 ||	Data/batch time: 0.2122/0.7249
[base_200_v2_rerun]	epoch/iter [0/500][200/500] ||	Loss: 4.8249, Top1_err: 94.8850, Top5_err: 84.0077 ||	Data/batch time: 0.1100/0.5656
[base_200_v2_rerun]	epoch/iter [0/500][300/500] ||	Loss: 4.6154, Top1_err: 92.6969, Top5_err: 79.0970 ||	Data/batch time: 0.0753/0.5131
[base_200_v2_rerun]	epoch/iter [0/500][400/500] ||	Loss: 4.4470, Top1_err: 90.6937, Top5_err: 75.1598 ||	Data/batch time: 0.0579/0.4870
[base_200_v2_rerun]	epoch/iter [0/500][499/500] ||	Loss: 4.3166, Top1_err: 89.1356, Top5_err: 72.1021 ||	Data/batch time: 0.0468/0.4732
Summary	epoch/iter [0/500] ||	TRAIN, Top1_err: 89.1356, Top5_err: 72.1021 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

model saved at result/base_200_v2_rerun/epoch_1.pth
[base_200_v2_rerun]	epoch/iter [1/500][0/500] ||	Loss: 3.7298, Top1_err: 81.4453, Top5_err: 58.3984 ||	Data/batch time: 17.3103/17.8009
[base_200_v2_rerun]	epoch/iter [1/500][100/500] ||	Loss: 3.6199, Top1_err: 79.8886, Top5_err: 55.8362 ||	Data/batch time: 0.1758/0.5954
[base_200_v2_rerun]	epoch/iter [1/500][200/500] ||	Loss: 3.5649, Top1_err: 79.0986, Top5_err: 54.5175 ||	Data/batch time: 0.0917/0.5055
[base_200_v2_rerun]	epoch/iter [1/500][300/500] ||	Loss: 3.5180, Top1_err: 78.3034, Top5_err: 53.4481 ||	Data/batch time: 0.0634/0.4738
[base_200_v2_rerun]	epoch/iter [1/500][400/500] ||	Loss: 3.4773, Top1_err: 77.6896, Top5_err: 52.5469 ||	Data/batch time: 0.0490/0.4574
[base_200_v2_rerun]	epoch/iter [1/500][499/500] ||	Loss: 3.4370, Top1_err: 77.0437, Top5_err: 51.6054 ||	Data/batch time: 0.0399/0.4462
Summary	epoch/iter [1/500] ||	TRAIN, Top1_err: 77.0437, Top5_err: 51.6054 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

[base_200_v2_rerun]	epoch/iter [2/500][0/500] ||	Loss: 3.3029, Top1_err: 74.8047, Top5_err: 50.5859 ||	Data/batch time: 17.1604/17.3870
[base_200_v2_rerun]	epoch/iter [2/500][100/500] ||	Loss: 3.1821, Top1_err: 72.7761, Top5_err: 45.8327 ||	Data/batch time: 0.1759/0.5988
[base_200_v2_rerun]	epoch/iter [2/500][200/500] ||	Loss: 3.1636, Top1_err: 72.4396, Top5_err: 45.4903 ||	Data/batch time: 0.0913/0.5065
[base_200_v2_rerun]	epoch/iter [2/500][300/500] ||	Loss: 3.1402, Top1_err: 72.1780, Top5_err: 45.0088 ||	Data/batch time: 0.0627/0.4733
[base_200_v2_rerun]	epoch/iter [2/500][400/500] ||	Loss: 3.1247, Top1_err: 71.8667, Top5_err: 44.7056 ||	Data/batch time: 0.0487/0.4612
[base_200_v2_rerun]	epoch/iter [2/500][499/500] ||	Loss: 3.1012, Top1_err: 71.4623, Top5_err: 44.2127 ||	Data/batch time: 0.0395/0.4493
Summary	epoch/iter [2/500] ||	TRAIN, Top1_err: 71.4623, Top5_err: 44.2127 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

[base_200_v2_rerun]	epoch/iter [3/500][0/500] ||	Loss: 2.9532, Top1_err: 66.4062, Top5_err: 43.5547 ||	Data/batch time: 13.7551/13.9563
[base_200_v2_rerun]	epoch/iter [3/500][100/500] ||	Loss: 2.9321, Top1_err: 68.6166, Top5_err: 40.7294 ||	Data/batch time: 0.1780/0.5977
[base_200_v2_rerun]	epoch/iter [3/500][200/500] ||	Loss: 2.9163, Top1_err: 68.1320, Top5_err: 40.3588 ||	Data/batch time: 0.0930/0.5046
[base_200_v2_rerun]	epoch/iter [3/500][300/500] ||	Loss: 2.9100, Top1_err: 68.1706, Top5_err: 40.2649 ||	Data/batch time: 0.0645/0.4742
[base_200_v2_rerun]	epoch/iter [3/500][400/500] ||	Loss: 2.8951, Top1_err: 67.8660, Top5_err: 39.8852 ||	Data/batch time: 0.0503/0.4595
[base_200_v2_rerun]	epoch/iter [3/500][499/500] ||	Loss: 2.8844, Top1_err: 67.6371, Top5_err: 39.6492 ||	Data/batch time: 0.0408/0.4490
Summary	epoch/iter [3/500] ||	TRAIN, Top1_err: 67.6371, Top5_err: 39.6492 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

[base_200_v2_rerun]	epoch/iter [4/500][0/500] ||	Loss: 2.8362, Top1_err: 65.6250, Top5_err: 37.8906 ||	Data/batch time: 16.9896/17.2721
[base_200_v2_rerun]	epoch/iter [4/500][100/500] ||	Loss: 2.7624, Top1_err: 65.2731, Top5_err: 37.0224 ||	Data/batch time: 0.2258/0.6650
