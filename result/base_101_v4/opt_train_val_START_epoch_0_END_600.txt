Experiment: base_101_v4
------------ Train and Test Options -----------------
KL_factor: 0.1
KL_manner: 1
add_cap_BN_relu: False
add_cap_dropout: False
b_init: zero
base_save_folder: result
batch_size_test: 128
batch_size_train: 128
beta1: 0.9
bigger_input: False
cap_N: 3
cap_model: v0
comp_cap: False
dataset: cifar10
debug_mode: False
depth: 14
device_id: 0,1
draw_hist: False
dropout_p: 0.2
experiment_name: base_101_v4
file_name: result/base_101_v4/opt_train_val_START_epoch_0_END_600.txt
fix_m: False
gamma: 0.1
less_data_aug: True
loss_form: margin
lr: 0.0001
manual_seed: -1
max_epoch: 600
measure_time: False
momentum: 0.9
multi_crop_test: False
no_visdom: False
non_target_j: False
num_workers: 2
optim: adam
phase: train_val
port_id: 9000
pre_ch_num: 256
primary_cap_num: 32
random_seed: 4906
route_num: 3
s35: True
save_epoch: 25
save_folder: result/base_101_v4
schedule: [150, 200, 250]
setting: top1
show_freq: 100
show_test_after_epoch: 100
squash_manner: paper
use_KL: False
use_cuda: True
use_instanceBN: False
use_multiple: False
weight_decay: 0.0005
------------------ End --------------------
DataParallel (
  (module): CapsNet(
    (tranfer_conv): Conv2d (3, 256, kernel_size=(9, 9), stride=(2, 2), padding=(1, 1))
    (tranfer_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (tranfer_relu): ReLU(inplace)
    (tranfer_conv1): Conv2d (256, 256, kernel_size=(3, 3), stride=(2, 2))
    (tranfer_bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
    (tranfer_relu1): ReLU(inplace)
    (max_pool): MaxPool2d(kernel_size=(5, 5), stride=(5, 5), dilation=(1, 1))
    (cap_layer): CapLayer(
      (W): Conv2d (256, 5120, kernel_size=(1, 1), stride=(1, 1), groups=32)
    )
  ), weights=((256, 3, 9, 9), (256,), (256,), (256,), (256, 256, 3, 3), (256,), (256,), (256,), (5120, 8, 1, 1), (5120,)), parameters=699648
)
Total param num # 2.668945 Mb

init learning rate 0.0001000000 at iter 0

[base_101_v4]	epoch/iter [0/600][0/391] ||	Loss: 0.8085, Top1_err: 90.6250, Top5_err: 53.1250 ||	Data/batch time: 0.0669/3.6411
[base_101_v4]	epoch/iter [0/600][100/391] ||	Loss: 0.7719, Top1_err: 81.5207, Top5_err: 29.3626 ||	Data/batch time: 0.0009/0.0848
[base_101_v4]	epoch/iter [0/600][200/391] ||	Loss: 0.6987, Top1_err: 80.3366, Top5_err: 28.6653 ||	Data/batch time: 0.0006/0.0672
[base_101_v4]	epoch/iter [0/600][300/391] ||	Loss: 0.6259, Top1_err: 77.4112, Top5_err: 24.9740 ||	Data/batch time: 0.0005/0.0613
[base_101_v4]	epoch/iter [0/600][390/391] ||	Loss: 0.5850, Top1_err: 75.1320, Top5_err: 22.5880 ||	Data/batch time: 0.0004/0.0587
Summary	epoch/iter [0/600] ||	TRAIN, Top1_err: 75.1320, Top5_err: 22.5880 ||	TEST, Top1_err: 100.0000, Top5_err: 100.0000 ||

model saved at result/base_101_v4/epoch_1.pth
[base_101_v4]	epoch/iter [1/600][0/391] ||	Loss: 0.4375, Top1_err: 66.4062, Top5_err: 13.2812 ||	Data/batch time: 0.0822/0.1237
