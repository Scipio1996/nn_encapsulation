import torch
import torch.nn as nn
from torch.autograd import Variable
from layers.misc import pad_matrix
from layers.cap_layer import conv_squash
EPS = 1e-20


class OptTrans(nn.Module):
    """
        Optimal transport
    """
    def __init__(self, ch_x, ch_y,
                 spatial_x, spatial_y, epsilon=1., L=10,
                 remove_bias=False,
                 group=1,
                 C_form='l2',
                 no_bp_P_L=False,
                 ):
        super(OptTrans, self).__init__()
        self.epsilon = 1./epsilon
        self.L = L
        self.remove_bias = remove_bias
        self.no_bp_P_L = no_bp_P_L
        # self.batch_size = batch_size
        # self.use_dynamic_a = use_dynamic_a

        # define G_net
        if spatial_x != spatial_y:
            stride, out_pad = 2, 1
        else:
            stride, out_pad = 1, 0
        self.G_net = nn.ModuleList([
            nn.ConvTranspose2d(ch_x, ch_y, groups=group, kernel_size=3,
                               padding=1, stride=stride, output_padding=out_pad),
            nn.BatchNorm2d(ch_y),
            nn.ReLU(),
        ])
        if group != 1:
            self.G_net.append(conv_squash(group))
        self.G_net = nn.Sequential(*self.G_net)

        # define critic
        self.critic = nn.Sequential(*[
            nn.Conv2d(ch_y, int(ch_y/4), kernel_size=3, padding=1, stride=2),
            nn.BatchNorm2d(int(ch_y/4)),
            nn.ReLU(),
            nn.Conv2d(int(ch_y/4), 1, kernel_size=3, padding=1, stride=2),
            nn.BatchNorm2d(1),
            nn.ReLU(),
        ])

    def forward(self, z, y):
        "x is generated by latent variable; y is ground truth"
        x = self.G_net(z)
        if self.remove_bias:
            loss = self._basic_compute_loss(x, y)
        else:
            loss = 2*self._basic_compute_loss(x, y) \
                    - self._basic_compute_loss(x, x) \
                    - self._basic_compute_loss(y, y)
        return loss

    def _basic_compute_loss(self, x, y):
        bs = x.size(0)
        x_ = self.critic(x)  # bs, 1, spatial_dim, spatial_dim
        y_ = self.critic(y)
        x_ = x_.view(bs, -1).unsqueeze(dim=2).repeat(1, 1, bs)
        y_ = y_.view(bs, -1).permute(1, 0).unsqueeze(dim=0)
        C = torch.norm((x_ - y_), p=2, dim=1)  # C: i, j where i, j are samples
        K = torch.exp(-self.epsilon*C)

        # Sinkhorn iterate
        # if bs < self.batch_size:
        #     K = pad_matrix(K, self.batch_size)
        # init b with ones
        # b = Variable(torch.ones(self.batch_size, 1)*(1./self.batch_size), requires_grad=True)
        # const = Variable(torch.ones(self.batch_size, 1)*(1./self.batch_size), requires_grad=False)
        b = Variable(torch.ones(bs, 1)*(1./bs), requires_grad=True)
        const = Variable(torch.ones(bs, 1)*(1./bs), requires_grad=False)
        # print('\n')
        for i in range(self.L):
            a = const / (torch.mm(K, b) + EPS)
            b = const / (torch.mm(K.permute(1, 0), a) + EPS)
            # print('L={:d}, a_min={:.6f}, a_max={:.6f}, a_mean={:.6f}, a_std={:.6f}'
            #       '\tb_min={:.6f}, b_max={:.6f}'.format(
            #         i, a.data.min(), a.data.max(),
            #         torch.mean(a).data[0], torch.std(a).data[0],
            #         b.data.min(), b.data.max()))

        # # in case of bs < self.batch_size
        # if bs < self.batch_size:
        #     a, b, K = a[0:bs], b[0:bs], K[0:bs, 0:bs]

        K = a*K*b.permute(1, 0)
        if self.no_bp_P_L:
            K = K.detach()
        # dot product of two matrices:
        # torch.sum(torch.mul())
        basic_loss = torch.dot(K.view(-1), C.view(-1))
        return basic_loss
