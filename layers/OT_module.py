import torch
import torch.nn as nn
from torch.autograd import Variable
from layers.cap_layer import conv_squash
EPS = 1e-20


class OptTrans(nn.Module):
    """Optimal transport unit"""
    def __init__(self, ch_x, ch_y,
                 spatial_x, spatial_y, epsilon=1., L=10,
                 remove_bias=False, group=1,
                 C_form='l2', no_bp_P_L=False, skip_critic=False
                 ):
        super(OptTrans, self).__init__()
        self.epsilon = 1./epsilon
        self.L = L
        self.remove_bias = remove_bias
        self.no_bp_P_L = no_bp_P_L
        self.C_form = C_form
        self.skip_critic = skip_critic

        # define G_net
        if spatial_x != spatial_y:
            stride, out_pad = 2, 1
        else:
            stride, out_pad = 1, 0
        self.G_net = nn.ModuleList([
            nn.ConvTranspose2d(ch_x, ch_y, groups=group, kernel_size=3,
                               padding=1, stride=stride, output_padding=out_pad),
            nn.BatchNorm2d(ch_y),
            nn.ReLU(),
        ])
        if group != 1:
            self.G_net.append(conv_squash(group))
        self.G_net = nn.Sequential(*self.G_net)

        # define critic
        if not self.skip_critic:
            self.critic = nn.Sequential(*[
                nn.Conv2d(ch_y, int(ch_y/4), kernel_size=3, padding=1, stride=2),
                nn.BatchNorm2d(int(ch_y/4)),
                nn.ReLU(),
                nn.Conv2d(int(ch_y/4), 1, kernel_size=3, padding=1, stride=2),
                nn.BatchNorm2d(1),
                nn.ReLU(),
            ])

    def forward(self, z, y):
        "x is generated by latent variable; y is ground truth"
        x = self.G_net(z)
        if self.remove_bias:
            loss = self._basic_compute_loss(x, y)
        else:
            loss = 2*self._basic_compute_loss(x, y) \
                    - self._basic_compute_loss(x, x) \
                    - self._basic_compute_loss(y, y)
        return loss

    def _basic_compute_loss(self, x, y):
        bs = x.size(0)
        if self.skip_critic:
            x = x.view(bs, -1)
            y = y.view(bs, -1)
        else:
            x = self.critic(x).view(bs, -1)         # bs, 1*spatial_dim*spatial_dim
            y = self.critic(y).view(bs, -1)

        if self.C_form == 'l2':
            x = x.unsqueeze(dim=2).repeat(1, 1, bs)
            y = y.permute(1, 0).unsqueeze(dim=0)
            C = torch.norm((x - y), p=2, dim=1)     # C: i, j where i, j are samples

        elif self.C_form == 'cosine':
            x /= (torch.norm(x, p=2, dim=1, keepdim=True) + EPS)
            y /= (torch.norm(y, p=2, dim=1, keepdim=True) + EPS)
            C = 1 - torch.mm(x, y.permute(1, 0))

        K = torch.exp(-self.epsilon*C)

        # Sinkhorn iterate
        b = Variable(torch.ones(bs, 1)*(1./bs), requires_grad=True)
        const = Variable(torch.ones(bs, 1)*(1./bs), requires_grad=False)

        for i in range(self.L):
            a = const / (torch.mm(K, b) + EPS)
            b = const / (torch.mm(K.permute(1, 0), a) + EPS)

        K = a*K*b.permute(1, 0)
        if self.no_bp_P_L:
            K = K.detach()

        basic_loss = torch.dot(K.view(-1), C.view(-1))
        return basic_loss
