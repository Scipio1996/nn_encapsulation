import torch
import torch.nn as nn
from torch.autograd import Variable
from layers.misc import pad_matrix
EPS = 1e-20


class OptTrans(nn.Module):
    """
        Optimal transport
    """
    def __init__(self, ch_x, ch_y,
                 spatial_x, spatial_y, epsilon=1., L=10, batch_size=128):
        super(OptTrans, self).__init__()

        self.epsilon = 1./epsilon
        self.L = L
        self.batch_size = batch_size
        # define G_net
        if spatial_x != spatial_y:
            stride, out_pad = 2, 1
        else:
            stride, out_pad = 1, 0
        self.G_net = nn.Sequential(*[
            nn.ConvTranspose2d(ch_x, ch_y, kernel_size=3, padding=1,
                               stride=stride, output_padding=out_pad),
            nn.BatchNorm2d(ch_y),
            nn.ReLU(),
        ])
        # define critic
        self.critic = nn.Sequential(*[
            nn.Conv2d(ch_y, int(ch_y/4), kernel_size=3, padding=1, stride=2),
            nn.BatchNorm2d(int(ch_y/4)),
            nn.ReLU(),
            nn.Conv2d(int(ch_y/4), 1, kernel_size=3, padding=1, stride=2),
            nn.BatchNorm2d(1),
            nn.ReLU(),
        ])

        self.tiny_linear_down = nn.Sequential(*[
            nn.Linear(batch_size, batch_size),
            nn.ReLU()
            # nn.Softmax(),
        ])
        self.tiny_linear_up = nn.Sequential(*[
            nn.Linear(batch_size, batch_size),
            nn.ReLU()
            # nn.Softmax(),
        ])

    def forward(self, z, y):
        "x is generated by latent variable; y is ground truth"
        x = self.G_net(z)
        # assert x.size() == y.size()
        loss = self._basic_compute_loss(x, y)
        # TODO: W(x, x) = 0
        # loss = 2*self._basic_compute_loss(x, y) - \
        #        self._basic_compute_loss(x, x) - \
        #        self._basic_compute_loss(y, y)
        return loss

    def _basic_compute_loss(self, x, y):
        bs = x.size(0)
        x_ = self.critic(x)  # bs, 1, spatial_dim, spatial_dim
        y_ = self.critic(y)
        x_ = x_.view(bs, -1).unsqueeze(dim=2).repeat(1, 1, bs)  # bs, spatial_dim**2, bs
        y_ = y_.view(bs, -1).unsqueeze(dim=2)  # bs, spatial_dim**2, 1
        C = torch.norm((x_ - y_), p=1, dim=1)  # C: i, j where i, j are samples
        K = torch.exp(-self.epsilon*C)

        # compute Sinkhorn loss
        if bs < self.batch_size:
            K = pad_matrix(K, self.batch_size)
        b = Variable(torch.ones(self.batch_size, 1)*(1./self.batch_size), requires_grad=True)
        const = Variable(torch.ones(self.batch_size, 1)*(1./self.batch_size), requires_grad=False)

        for i in range(self.L):
            # TODO: see the changes of a and b as L iterates
            input_b = self.tiny_linear_down(b.permute(1, 0)).permute(1, 0)
            # input_b = torch.exp(input_b)
            a = const / (torch.mm(K, input_b) + EPS)

            input_a = self.tiny_linear_up(a.permute(1, 0)).permute(1, 0)
            # input_a = torch.exp(input_a)
            b = const / (torch.mm(K.permute(1, 0), input_a) + EPS)

            # print('L={:d}, a_min={:.6f}, a_max={:.6f}, a_mean={:.6f}, a_std={:.6f}'
            #       '\tb_min={:.6f}, b_max={:.6f}'.format(
            #     i, a.data.min(), a.data.max(), torch.mean(a).data[0], torch.std(a).data[0],
            #     b.data.min(), b.data.max()))

        # in case of bs < self.batch_size
        if bs < self.batch_size:
            a, b, K = a[0:bs], b[0:bs], K[0:bs, 0:bs]

        K = a*K*b.permute(1, 0)
        # dot product of two matrices:
        # torch.sum(torch.mul())
        basic_loss = torch.dot(K.view(-1), C.view(-1))
        return basic_loss
